\chapter{Introduction}
\label{ch:introduction}

The OpenStreetMap project’s database stores a wealth of geographical information
about the world ranging from map-typical features like borders, streets and
buildings to detailed information about points of interest like shops, sights
and recreational areas. However, extracting specific information – such as the
answer for the simple natural language question \nl{Which Italian restaurants in
  Berlin are wheelchair-accessible?} – requires either purpose-built tooling or
knowledge of custom OpenStreetMap query languages. A natural language interface
to the database would make the available information more accessible.

Important groundwork for such a natural language interface was laid by
\textcite{haas-2016}, who developed a simple machine-readable query language
(MRL) for OpenStreetMap and also released the dataset \emph{\nlmaps{}}, which
maps natural language (NL) queries to their corresponding counterpart in that
query language. This dataset makes it possible to train a semantic parser that
parses NL into MRL queries.

Despite various models producing good results on the \nlmaps{} test set, their
performance does not suffice for practical usage on new real world queries.
Investigating that problem, this thesis reviews previous work on \nlmaps{} and
reveals shortcomings in the published dataset. In order to improve on it, the
existing dataset is overhauled by eliminating some of the identified
shortcomings and by extending it through the use of probabilistic templates to
include more diverse queries on both the natural language and the
machine-readable language side.

The improved \nlmaps{} dataset is used to train an improved parsing model, which
is exposed via a new web interface so that it can be used for both asking
queries and correcting a parse if the model gets it wrong. In order to further
improve the model, the system is able to directly learn from the corrected
queries using an online learning technique.

The new web interface is employed in an annotation experiment with users from
various backgrounds who use it to ask new queries and to correct errors. The
data collected in this way is used to test the online learning setup and is also
released as a new \nlmaps{} dataset.

The web interface and all datasets published in this work are available at
{\small\url{https://nlmaps.gorgor.de/}}. The following code repositories are
associated with this thesis and are tagged with the Git tag
{\small\texttt{thesis}} to mark the state they are in as of the thesis’s
publishing date:

\begin{itemize}
\item {\small\url{https://gitlab.cl.uni-heidelberg.de/will/nlmapsweb/}}: The web
  interface.
\item {\small\url{https://gitlab.cl.uni-heidelberg.de/will/joeynmt-server/}}:
  The backend server handling parsing and training the parser.
\item {\small\url{https://gitlab.cl.uni-heidelberg.de/will/nlmaps-tools/}}: A
  package containing various tools for working with \nlmaps{} datasets, for
  generating the \nlmapsthree{} version and for interpreting an MRL query and
  retrieving an answer.
\item {\small\url{https://gitlab.cl.uni-heidelberg.de/will/nlmaps-ma/}}: This
  thesis along with scripts for data analysis and plotting.
\item {\small\url{https://github.com/Simon-Will/joeynmt}}: A slightly modified
  version of Joey NMT \parencite{kreutzer-2019}.
\item {\small\url{https://github.com/Simon-Will/osm-python-tools}}: A slightly
  modified version of OSMPythonTools \parencite{mocnik-2017}. A pull request to
  the upstream repository is still open.
\end{itemize}

%%% Local Variables:
%%% coding: utf-8
%%% mode: latex
%%% TeX-engine: luatex
%%% TeX-parse-self: t
%%% TeX-command-extra-options: "-shell-escape"
%%% TeX-master: "../thesis"
%%% End: