\chapter{Experiments}
\label{ch:experiments}

Our goal is simulating different strategies of online learning for the new web
interface. Before we can do that, we first pre-train parsers in
Section~\ref{sec:pre-training} on the existing datasets \nlmapstwo{} and the
variations we introduced in \nlmapsthree{} and evaluate whether the dataset
extensions yield any improvement in parser quality. In
Section~\ref{sec:annotation}, we then hire annotators to use our web interface
to ask NL queries and correct the MRL parse if it is incorrect, thus collecting
a new dataset consisting of real user queries. Finally, in
Section~\ref{sec:online-simulation} we use the newly acquired dataset for
evaluating various online learning setups.

For all our experiments, we use the same model architecture: A character-based
one-layer bidirectional GRU encoder-decoder \parencite{cho-2014} model with
attention\parencite{bahdanau-2015}. The dimension of both source and target
embeddings is \num{620}, the encoder layer size is \num{500}, the decoder layer
size is \num{1000} and we don’t use dropout. This model configuration is adopted
from \textcite{staniek-2020}.

\section{Training on \nlmapstwo{} and \nlmapsthree{}}
\label{sec:pre-training}

While \textcite{staniek-2020} trained his model on the \nlmapstwo{} dataset for
\num{100} epochs (of \num{16172} instances each), we train our models in this
section for a shorter time, which is still enough for sufficient convergence:
The model on \nlmapstwoone{} is trained for \num{60} epochs (of \num{15113}
instances each), while the models on variations of \nlmapsthree{} are trained
for \num{30} epochs (of \num{30226} instances each). All models are trained with
the Adam optimizer (\(\beta_1 = 0.9\) and \(\beta_2 = 0.999\)) and a learning
rate of \num{0.0002}.

Table~\ref{tab:pre-trained-performance} shows the results of testing the models
on the different variations of \nlmaps{} datasets. Recall: \nlmtwoone{} is the
result of fixing issues in the MRL queries of \nlmtwo{}, but has identical NL
queries; \nlmthreea{} is purely generated with probabilistic templates and
\nlmthreeb{} is its counterpart with added noise; \nlmthreenormal{} is
\nlmtwoone{} + \nlmthreea{} and \nlmthree{} is \nlmtwoone{} + \nlmthreeb{}. The
datasets \nlmfourraw{} and \nlmfour{} are introduced in
Section~\ref{sec:annotation} and are the user-supplied MRL-NL pairs with some
corrections applied to them in \nlmfour{}. Even though the last two datasets
were not yet available when the models were pre-trained, we still evaluate the
pre-trained models on them in this section because we consider the results on
these user-supplied queries the most relevant.

\begin{table}[h]
  \centering
  \begin{tabular}{lcccccccc}
    \toprule
    \diagbox{Train}{Test} & \nlmtwo{} & \nlmtwoone{} & \nlmthreea{} & \nlmthreeb{} & \nlmthreenormal{} & \nlmthree{} & \nlmfourraw{} & \nlmfour{}\\
    \midrule
    \textcite{staniek-2020} & \bfnum{0.898} & \num{0.844} & \num{0.039} & \num{0.033} & \num{0.441} & \num{0.439} & \num{0.050} & \num{0.052}\\
    \nlmtwoone{} & \num{0.783} & \num{0.913} & \num{0.034} & \num{0.029} & \num{0.474} & \num{0.471} & \num{0.070} & \num{0.069}\\
    \nlmthreea{} & \num{0.224} & \num{0.251} & \bfnum{0.987} & \num{0.789} & \num{0.618} & \num{0.519} & \num{0.217} & \num{0.223}\\
    \nlmthreeb{} & \num{0.372} & \num{0.424} & \num{0.976} & \bfnum{0.884} & \num{0.700} & \num{0.656} & \num{0.226} & \num{0.233}\\
    \nlmthreenormal{} & \num{0.790} & \bfnum{0.919} & \num{0.978} & \num{0.792} & \bfnum{0.948} & \num{0.857} & \bfnum{0.307} & \bfnum{0.311}\\
    \nlmthree{} & \num{0.787} & \num{0.913} & \num{0.950} & \num{0.834} & \num{0.931} & \bfnum{0.874} & \num{0.281} & \num{0.289}\\
    \bottomrule
  \end{tabular}
  \caption[Performance of pre-trained parsers]{Performance of pre-trained
    parsers.}
  \label{tab:pre-trained-performance}
\end{table}

Unsurprisingly, the model by \textcite{staniek-2020}, which was trained on
\nlmapstwo{}, performs best on the corresponding test set with an accuracy of
\SI{89.8}{\%}. The model trained on \nlmtwoone{}, which contains the fixes of
tag usage and inconsistencies described in Chapter~\ref{ch:nlmaps-improvement},
achieves a higher accuracy on its corresponding test set with \SI{91.3}{\%}.
This is to be expected since the resolution of inconsistencies in MRL structure
and tag usage makes a new part of the dataset accessible for confident
predictions in the first place. These two models’ performance drops dramatically
on new datasets. An error analysis (see Figure~\ref{fig:v21-errors}) shows that
not only does the model trained on \nlmtwoone{} make an error in the
\mrl{target_nwr} operator in over \SI{70}{\%} of queries from the \nlmapsfour{}
test set, the recognized \mrl{area} is false in more than half the queries, as
well.

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{\input{fig/n21_n4_errors.pgf}}
  \caption[Errors when pre-trained on \nlmtwoone{}]{Percentage of queries with an
    error in a specific operator. The model was pre-trained on \nlmapstwoone{}
    and tested on \nlmapsfour{}.}
  \label{fig:v21-errors}
\end{figure}

The models trained on the purely synthetic training sets of \nlmthreea{} and
\nlmthreeb{} achieve an extremely high accuracy of around \SI{98}{\%} on the
test set of \nlmthreea{}. The model trained on the non-noisy \nlmthreea{} turns
out to be less robust on the noisy \nlmthreeb{} test set with a performance drop
of almost \SI{20}{\%} whereas the accuracy of the model trained on \nlmthreeb{}
only drops by around \SI{9}{\%}. More interestingly, the \nlmthreeb{} model
significantly outperforms the \nlmthreea{} model on the \nlmtwoone{} test set,
as well, which suggests that the added noise serves as a means of regularization
and avoids overfitting on the synthetic queries. When confronted with the real
queries in \nlmapsfour{}, both models’ accuracy again drops starkly to around
\SI{23}{\%} – with the noisy model still performing slightly better than its
non-noisy counterpart. As shown in Figure~\ref{fig:v3b-errors}, the improvement
with respect to the model trained on \nlmtwoone{} is mostly due to the greater
variety of location names (cf. Section~\ref{sec:little-location-variety}), which
significantly reduces the number of errors in the \mrl{area} and
\mrl{center_nwr} operators. The main source of errors remains choosing an
incorrect set of OSM tags in the \mrl{target_nwr} operator.

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{\input{fig/n3b_n4_errors.pgf}}
  \caption[Errors when pre-trained on \nlmthreeb{}]{Percentage of queries with an
    error in a specific operator. The model was pre-trained on \nlmapsthreeb{}
    and tested on \nlmapsfour{}.}
  \label{fig:v3b-errors}
\end{figure}

Combining \nlmtwoone{} and the new synthetic data from \nlmthreea{} or
\nlmthreeb{} yields further improvement as evidenced by the performance of the
models trained on \nlmthreenormal{} and \nlmthree{}. They perform well on
\nlmtwoone{} as well as on the variations of \nlmthree{}, but the accuracy on
the noisy versions is still noticeably smaller than on their non-noisy
counterparts. While the comparison of the two models trained on \nlmthreea{} and
\nlmthreeb{} showed a clearly superior performance of the model trained on the
noisy data in all variations of \nlmtwo{} and \nlmthree{} except \nlmthreea{},
this is not observed when comparing the two models trained on \nlmthreenormal{}
and \nlmthree{}. Instead, they have a similar accuracy on all test sets with the
model trained on \nlmthreenormal{} being slightly superior on the non-noisy
datasets and the model trained on \nlmthree{} being slightly superior on the
noisy ones. Testing on the real user queries from \nlmfour{} reveals that the
model trained on the non-noisy data actually performs better in the end.
However, since \nlmfour{} was not available during the pre-training phase, at
that point the two models seemed comparable and our assumption was that the
model trained on \nlmthreeb{} would prove more robust against typing errors.

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{\input{fig/n3_n4_errors.pgf}}
  \caption[Errors when pre-trained on \nlmthree{}]{Percentage of queries with an
    error in a specific operator. The model was pre-trained on \nlmapsthree{}
    and tested on \nlmapsfour{}.}
  \label{fig:v3-errors}
\end{figure}

Figure~\ref{fig:pre-trained-learning-curve} shows the models’ learning curves on
the development set of \nlmthree{} during pre-training. While all models
converge fairly quickly, it can be seen that the model trained on \nlmtwoone{}
converges somewhat faster than the other models. This might again be due to the
simpler location names, which can be memorized by the model, while the other
models have to learn to copy arbitrary location names – including some that are
very long or contain rare characters.

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{\input{fig/n3_learning_curve.pgf}}
  \caption[Pre-training learning curves]{Learning curves on the development set
    of \nlmapsthree{}.}
  \label{fig:pre-trained-learning-curve}
\end{figure}

\section{Annotation for New Dataset}
\label{sec:annotation}

Training and evaluating useful \nlmaps{} models without having a dataset of
actual user queries is problematic. For this reason, an annotation experiment is
conducted, during which several people use our web interface presented in
Chapter~\ref{ch:web-interface} to issue new NL queries. They are tasked to
confirm the model’s predicted MRL query if they deem it correct or to correct it
if they consider it false. Before beginning the task, the annotators complete a
tutorial, which explains the basics of OSM, the capabilities of the MRL language
and the usage of the web interface.

The call for participation in the annotation study was distributed through
various OSM-typical channels including the \emph{talk} mailing
list\footciteonline{osm-talk} and the OSM
subreddit\footciteonline{osm-subreddit}, which yielded the highest response.
Albeit slightly eurocentric, the participants stem from a reasonably diverse set
of countries and have diverse native languages. In contrast, all of the acquired
annotators report their gender as male, which is reflective of the overall OSM
demographic since only between \SI{2}{\%} and \SI{5}{\%} of OSM contributors are
estimated to be female
\parencites{budhathoki-2010}{stark-2010}{lechner-2011}{klettner-2013}{das-2019}.
This is problematic as it has the potential to lead to a bias in the dataset. In
future annotation projects, this effect should be mitigated by targeted
advertising in female mapping communities such as
GeoChicas.\footciteonline{geochicas}

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{tabular}[t]{rllll}
      \toprule
      \#Annotations & Nationality & Native Language & OSM Experience & Gender\\
      \midrule
      \num{442} & Poland & Polish & Extensive & Male\\ % Mateusz
      \num{414} & Germany & German & Extensive & Male\\ % Martin
      \num{405} & Turkey & Turkish & Medium & Male\\ % Ali Y
      \num{404} & UK & English & Medium & Male\\ % Jake
      \num{404} & Hungary & Hungarian & Extensive & Male\\ % Benjámin Z
      \num{400} & India & Hindi & Little & Male\\ % Aksha
      \num{400} & Turkey & Turkish & Little & Male\\ % Ali A
      \num{393} & Germany & German & Little & Male\\ % David
      \num{318} & Germany & German & Extensive & Male\\ % Simon
      \num{253} & Brazil & Portuguese & Little & Male\\ % Thales
      \num{253} & Nepal & Nepalese & Extensive & Male\\ % Rabin
      \num{26} & Philippines & English, Filipino, Cebuano & Extensive & Male\\ % Erwin
      \num{40} & \multicolumn{4}{l}{\emph{Various untracked people without login}}\\
      \bottomrule
    \end{tabular}}
  \caption[Annotator profiles]{Information about annotators. Everything but the
    number of annotations is self-reported by the annotators.}
  \label{tab:annotator-profiles}
\end{table}

The annotation guidelines explain to the annotators that their queries should be
linguistically diverse, cover a large part of OSM tags and cover more useful
tags in more detail. To ensure that the annotators sufficiently cover the most
important tags, we choose a set of tags (e.g.\ \osmtag{amenity=cafe} and
\osmtag{wheelchair=yes}) and keys (e.g.\ \osmtag{shop} and \osmtag{leisure}) for
which each annotator must issue a minimum number of queries. See the full
guidelines in Appendix~\ref{ch:annotation-guidelines} for details. The
annotators can monitor their annotation progress via an overview page in the web
interface, which is shown in Figure~\ref{fig:annotation-progress}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{fig/annotation_progress.png}
  \caption[Annotation progress overview]{Overview of an annotator’s annotation
    progress.}
  \label{fig:annotation-progress}
\end{figure}

During the annotation, the parser model used by the web interface is initially
the model from Section~\ref{sec:pre-training} that is pre-trained on
\nlmapsthree{}. Due to technical problems however, no systematic online learning
happened throughout the annotation experiment because of various crashes. The
result is a dataset where the annotators made their corrections based on
predictions made by a model that had learned from some, but not all annotations.

In total, the annotators issued \num{4152} NL queries. For \num{92} of those,
they could not give an MRL, mostly because the queries were not expressible by
an MRL query (e.g.\ \nl{cities without hospitals in Poland}), leaving \num{4060}
NL-MRL pairs. Since \textcite{staniek-2020} correctly criticized that
\nlmapstwo{} has queries in its development and test sets that appear
identically in the training set if location names are masked (cf.\
Section~\ref{sec:train-test-resemblance}), we identify such sets of identical
queries in the remaining \num{4060} NL-MRL pairs and delete all but one in each
set, which leaves \num{3773} NL-MRL pairs.

Furthermore, we observe that users tend to reuse the same areas many times.
E.g., a user from Istanbul will ask lots of questions about Istanbul, which
leads to the problem of lacking variety in location names that was already
identified as a problem in Section~\ref{sec:little-location-variety}. In order
to avoid this problem in our new dataset, the web interface automatically
replaces a location name (anything with OSM tag \osmtag{name=*}) if it has
occurred three or more times in that user’s queries already. The original query
is of course saved separately, as well. The location names used for replacement
are selected from the same large list of areas and points of interest that is
also used for generating the \nlmapsthree{} dataset. The resulting dataset of
\num{3773} NL-MRL pairs with replacement of some location names is called
\nlmapsfourraw{}.

Finally, we notice some errors in the MRLs the annotators created. E.g., for the
NL query \nl{show me restaurants with limited wheelchair access in gmina
  Piaseczno}, the annotated MRL query erroneously uses \mrl{qtype(findkey(}
\mrl{'wheelchair'))} (which informs about the wheelchair accessibility status of
returned objects) instead of actually \emph{selecting} for wheelchair status by
using \mrl{keyval('wheelchair',} \mrl{'limited')} in the \mrl{nwr} operator.
Similar queries by the same annotator where they didn’t make that mistake show
that they understood the issue and may just have been unconcentrated in this
case. Other examples include using \osmtag{diet=chinese} instead of
\osmtag{cuisine=chinese} or \osmtag{shop=fuel} instead of \osmtag{amenity=fuel},
both of which can be attributed to a lack of concentration, which is not
surprising in a repetitive annotation task.

Besides these obvious errors, there is a separate and more frequent issue with
ambiguous NL queries. E.g., the NL query \nl{count the fireplaces in the
  Naturpark Schönbuch} is annotated to select objects tagged as
\mrl{keyval('leisure','firepit')}, which ignores fireplaces that are mapped with
the key \osmtag{fireplace=yes} on picnic sites. Another annotator issues a
similar NL query, but creates an MRL which selects objects tagged as
\mrl{or(keyval('fireplace','yes'),} \mrl{keyval('leisure',} \mrl{'firepit'))}.
While the second query is more inclusive and overall more fitting for the NL
query, there is a valid reason to exclude \osmtag{fireplace=yes}: It can also be
used on wilderness huts to indicate that they feature an \emph{indoor}
fireplace, which may not be what the user has in mind. This is one of numerous
cases where it is not at all obvious what the correct MRL query for an NL query
should be.

Since our goal is creating a dataset that is as useful as possible for training
an NLMaps model which makes sensible and consistent predictions, we fix the
obvious errors and we also adjust MRLs where there are diverging MRL
interpretations for similar NL queries in the dataset. This is judged by the
author Simon Will based on the MRLs created by all the annotators. As a result
of this resolution, \num{377} MRLs of the \num{3773} instances are modified to
create the dataset called \nlmapsfour{}.

Both \nlmfourraw{} and \nlmfour{} are split into training, development and test
sets containing \num{2264}, \num{754} and \num{755} instances respectively.
Table~\ref{tab:v2-v3-v4-overview} compares the new datasets with \nlmtwoone{}
and \nlmthreeb{} and shows that the user queries are actually shorter and have a
smaller entropy rate than the queries generated by our probabilistic templates.

\begin{table}[h]
  \centering
  \begin{tabular}{lrrrr}
    \toprule
    Measure & \nlmtwoone{} & \nlmthreeb{} & \nlmfourraw{}/\nlmfour{}\\
    \midrule
    Instances & \num{28609} & \num{53500} & \num{3773}\\
    Conditional Entropy & \num{2.11} & \num{2.93} & \num{2.68}\\
    Avg. Tokens per NL & \num{6.98} & \num{10.69} & \num{8.37}\\
    \bottomrule
  \end{tabular}
  \caption[Dataset statistics with \nlmapsfour{}]{Comparison of dataset
    statistics. Entropy rates are estimated by the conditional entropy
    trigrams.}
  \label{tab:v2-v3-v4-overview}
\end{table}

Probably due to the influence of the annotation guidelines, the most common tags
in \nlmapsfour{} are \osmtag{wheelchair=yes}, \osmtag{amenity=restaurant} and
\osmtag{amenity=cafe}, which are used in over \num{100} queries each. What makes
the dataset especially challenging, is the long tail of rarely used tags:
\num{310} tags are used only once and \num{489} are used at most three times.
Figure~\ref{fig:tag-distribution} reveals two insights about how the tag
distribution in \nlmapsfour{} compares to that of the previous datasets: The
synthetic nature of \nlmapsthreeb{} and the larger part of \nlmapstwoone{} makes
it feasible to generate an arbitrary number for each tag, which is of course not
possible when manually issuing queries. At the same time, the manual processs is
more creative resulting in a larger number of different tags. Note that even
though the number of instances in \nlmapsfour{} is only \SI{7}{\%} of that in
\nlmapsthree{}, it still contains \num{1.3} times as many different tags.

In conclusion, the annotation process was unfortunately only done by male
participants and the question of how to handle diverging MRL queries for similar
NL queries deserves further research. Nevertheless, the result is a challenging
dataset with a large number of tags and more \emph{real} user queries than any
of the previous \nlmaps{} datasets.

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{\input{fig/tag_counts.pgf}}
  \caption[Tag distribution]{Tag distribution in different \nlmaps{} datasets.}
  \label{fig:tag-distribution}
\end{figure}

\section{Online Learning Simulation}
\label{sec:online-simulation}

In this section, we train various models on the new \nlmapsfour{} dataset and
investigate which learning setup is best suited for online learning in the web
interface. The model trained on the large \nlmapsthree{} dataset is used as a
starting point for all models in this section.

First, we establish an upper boundary for the fine-tuning on \nlmfour{} in a
regular batch-based offline learning experiment. In the setup called \nlmthree{}
\textrightarrow{} \nlmfour{} in Table~\ref{tab:fine-tuned-performance} the model
is fine-tuned on \nlmfour{} for \num{20} epochs with a batch size of \num{10}.
While that model achieves significant accuracy gains on \nlmfour{}, its
performance on the old dataset \nlmthree{} drops starkly. Therefore, another
model is trained for \num{20} epochs (of \nlmfour{}) with batches made up of
\num{5} instances from \nlmfour{} and \num{5} instances of \nlmthree{}. This
model achieves the highest accuracy on \nlmfour{} in this thesis: \SI{58.8}{\%}.
The learning curve shown in Figure~\ref{fig:v3-v4-learning-curve} shows that the
performance on \nlmthree{} remains fairly stable throughout the training while
the performance on the new \nlmfour{} is improved. Figure~\ref{fig:v3-v4-errors}
shows that errors are still predominantly caused by selecting the wrong tags to
query for, even though the tag error rate is greatly reduced with respect to the
pre-training model (compare Figure~\ref{fig:v3-errors}).

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{\input{fig/n3_arc_ratio05_learning_curve.pgf}}
  \caption[\nlmthree{}+\nlmfour{} learning curve]{Learning curve of model
    pre-trained on \nlmapsthree{} and fine-tuned on a mix of \nlmapsthree{} and
    \nlmapsfour{}.}
  \label{fig:v3-v4-learning-curve}
\end{figure}

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{\input{fig/n3_arc_ratio05_errors.pgf}}
  \caption[Errors after fine-tuning]{Percentage of queries with an
    error in a specific operator. The model was pre-trained on \nlmthree{},
    fine-tuned on \nlmthree{} and \nlmfour{} and tested on \nlmfour{}.}
  \label{fig:v3-v4-errors}
\end{figure}

\begin{table}[h]
  \centering
  \begin{tabular}{lccccc}
    \toprule
    \diagbox{Train}{Test} & \nlmtwoone{} & \nlmthreenormal{} & \nlmthree{} & \nlmfourraw{} & \nlmfour{}\\
    \midrule
    \nlmthree{} & \num{0.913} & \bfnum{0.931} & \bfnum{0.874} & \num{0.281} & \num{0.289}\\
    \midrule
    \nlmthree{} \textrightarrow{} \nlmfour{} & \num{0.836} & \num{0.852} & \num{0.792} & \num{0.518} & \num{0.554}\\
    \nlmthree{} \textrightarrow{} \nlmthree{}/\nlmfour{} & \bfnum{0.914} & \num{0.923} & \num{0.865} & \bfnum{0.548} & \bfnum{0.588}\\
    \midrule
    \nlmthree{} \textrightarrow{} \nlmfour{} Adadelta 1-0-0 Iter: 1 & \num{0.817} & \num{0.835} & \num{0.774} & \num{0.376} & \num{0.391}\\
    \nlmthree{} \textrightarrow{} \nlmthree{}/\nlmfour{} Adadelta 1-0-5 Iter: 1 & \num{0.911} & \num{0.927} & \num{0.871} & \num{0.354} & \num{0.366}\\
    \nlmthree{} \textrightarrow{} \nlmthree{}/\nlmfour{} Adadelta 1-4-5 Iter: 1 & \num{0.905} & \num{0.924} & \num{0.866} & \num{0.381} & \num{0.396}\\
    \nlmthree{} \textrightarrow{} \nlmthree{}/\nlmfour{} Adadelta 1-4-5 Iter: 5 & \num{0.912} & \num{0.930} & \num{0.871} & \num{0.430} & \num{0.448}\\
    \midrule
    \nlmthree{} \textrightarrow{} \nlmfour{} Adam 1-0-0 Iter: 1 & \num{0.565} & \num{0.601} & \num{0.552} & \num{0.384} & \num{0.397}\\
    \nlmthree{} \textrightarrow{} \nlmthree{}/\nlmfour{} Adam 1-0-5 Iter: 1 & \num{0.886} & \num{0.907} & \num{0.847} & \num{0.430} & \num{0.450}\\
    \nlmthree{} \textrightarrow{} \nlmthree{}/\nlmfour{} Adam 1-4-5 Iter: 1 & \num{0.887} & \num{0.901} & \num{0.841} & \num{0.486} & \num{0.510}\\
    \nlmthree{} \textrightarrow{} \nlmthree{}/\nlmfour{} Adam 1-4-5 Iter: 5 & \num{0.859} & \num{0.827} & \num{0.774} & \num{0.499} & \num{0.530}\\
    \midrule
    \nlmthree{} \textrightarrow{} \nlmthree{}/\nlmfourraw{} Adam 1-4-5 Iter: 1 & \num{0.880} & \num{0.884} & \num{0.831} & \num{0.483} & \num{0.490}\\
    \bottomrule
  \end{tabular}
  \caption{Performance of fine-tuned parsers.}
  \label{tab:fine-tuned-performance}
\end{table}

\begin{figure}[p]
  \centering
  \resizebox{0.9\textwidth}{!}{\input{fig/online_1_arc.pgf}}
  \caption[1-iter learning curve on \nlmfour{}]{Learning curve on development
    set of \nlmapsfour{} during online simulation with one iteration per
    instance.}
  \label{fig:online-1-v4-learning-curve}
\end{figure}

\begin{figure}[p]
  \centering
  \resizebox{0.9\textwidth}{!}{\input{fig/online_1_n3.pgf}}
  \caption[1-iter learning curve on \nlmthree{}]{Learning curve on development
    set of \nlmapsthree{} during online simulation with one iteration per
    instance.}
  \label{fig:online-1-v3-learning-curve}
\end{figure}

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{\input{fig/online_5_arc.pgf}}
  \caption[5-Iter learning curve on \nlmfour{}]{Learning curve on development
    set of \nlmapsfour{} during online simulation with five iterations per
    instance.}
  \label{fig:online-5-v4-learning-curve}
\end{figure}

The next experiment is a simulation of online learning: The model trained on
\nlmapsthree{} is fine-tuned on \nlmapsfour{} by making one pass through the
\nlmfour{} training set and performing one gradient descent step per instance.
In the 1-0-0 variant of the experiment, the \enquote{minibatch} for calculating
the gradient consists of only this instance. In the 1-0-5 variant, 5 instances
sampled from \nlmapsthree{} are added to the minibatch. In the 1-0-5 variant, a
further 4 instances sampled from the part of \nlmfour{} seen at that point are
added to the minibatch, which means that the minibatches consist of 5 instances
from \nlmthree{} and 5 instances from \nlmfour{}, just like in the previous
offline experiment. Another variation of the experiment is iterating five times
instead of once for each instance in \nlmfour{}, which is indicated by the
\enquote{Iter: 5} in Table~\ref{tab:fine-tuned-performance}. Note that while the
one instance is used in all five iterations in this case, the other samples
change with each minibatch. The exact meaning of these four parameters \(n_F\),
\(n_M\), \(n_T\) and \(I\) is described more formally in
Algorithm~\ref{alg:async-online-learning} at the end of
Chapter~\ref{ch:web-interface}.

Since previous work suggests that Adadelta performs as well as
\parencite{peris-2017} or even better than \parencites{turchi-2017}{peris-2019}
Adam for online learning on single instances, the first experiments are
conducted using Adadelta with the learning rate set to \num{0.01} as done by
\textcite{peris-2019}. The results show that the simple 1-0-0 variant increases
the accuracy on \nlmfour{} by over \SI{10}{\%}, but at the same time the
accuracy on \nlmthree{} degrades by \SI{10}{\%}. Adding instances sampled from
\nlmthree{} to the minibatch almost eradicates this problem, but also impedes
adaptation as evidenced by an increase of accuracy on \nlmfour{} by only
\SI{7}{\%}. Adding further memorized \nlmfour{} instances in the 1-4-5 variant
evidently rectifies this problem. By performing 5 iterations per instance, the
accuracy on \nlmfour{} can even be increased by \SI{15}{\%} with respect to the
pre-trained model.

Substituting Adam (again with a learning rate of \num{0.0002}) for Adadelta
yields little improvement in the 1-0-0 variant. In fact, there is an extreme
decrease in accuracy to only \SI{55.2}{\%} on the original \nlmthree{} data,
which is illustrated well in the learning curve in
Figure~\ref{fig:online-1-v3-learning-curve}. Again, adding the \nlmthree{}
instances to the batch alleviates this problem, but not as effectively as with
Adadelta. However, this stabilization helps the Adam 1-0-5 variant to reach an
\nlmfour{} accuracy of \SI{45.0}{\%}, significantly outperforming its Adadelta
counterpart. The memory of previous \nlmfour{} instances further increases the
accuracy to \SI{51.0}{\%}. Performing 5 iterations per instance yields
another accuracy increase of \SI{2}{\%}, but in the case of Adam this comes with
a \SI{7}{\%} decrease on \nlmthree{}.

While all of the online learning setups are able to increase the accuracy on the
new \nlmapsfour{} dataset, it is a challenge not to degrade the model on the
original data. It proves beneficial to include instances from the original data
as well as from memorized new instances in each update in order to make the
largest gains in accuracy on the new data while preserving old performance as
well as possible. Overall, Adadelta is the more conservative optimizer in this
regard, which comes at a significantly worse ability to adapt to the new data.
In contrast, Adam adapts better, but is very prone to overfitting on the new
data when not provided with original data. It also turns out not to have any
advantage over Adadelta when the minibatch consists only of the one new
instance, which is in line with findings in previous work. Furthermore, using a
single instance in five consecutive batches leads to overfitting even when
original data is added. Most likely, this is due to Adam’s first-order momentum
which builds up over these batches.

Overall, the online learning is successful, but still not as effective as
traditional offline learning from a mix of \nlmthree{} and \nlmfour{}, which
achieves an accuracy which is higher by \SI{5.8}{\%}. But note that the offline
learning was run for \num{20} epochs while the online learning simulations made
only one pass over the training data. Compare the number of steps in
Figures~\ref{fig:v3-v4-learning-curve} and \ref{fig:online-1-v4-learning-curve}
for this.

\FloatBarrier
\section{Qualitative Longitudinal Analysis}

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{\input{fig/online_analysis.pgf}}
  \caption[Longitudinal analysis]{Longitudinal analysis of predictions for
    twenty randomly selected tags during the online learning simulation on
    \nlmfour{}. \textcolor{mplGreen}{Green crosses} mean that the model’s
    prediction is completely correct, \textcolor{mplBlue}{blue circles} mean
    that the tag in question was correctly predicted but there is some other
    error with the prediction and \textcolor{mplRed}{red triangles} mean that
    the tag in question is not present in the prediction (implying also that the
    prediction is incorrect).}
  \label{fig:online-analysis}
\end{figure}

In order to analyze what was learned by adapting on \nlmfour{}, we conduct a
longitudinal analysis by observing the predictions of the model called
\enquote{\nlmthree{} \textrightarrow{} \nlmthree{}/\nlmfour{} Adam 1-4-5 Iter:
  1} in Table~\ref{tab:fine-tuned-performance} made during the online learning
pass \emph{before} each update step. More specifically, we track the performance
on twenty tags that were randomly sampled from all tags occurring at least twice
in the \nlmfour{} training set. Figure~\ref{fig:online-analysis} plots the
success of the predictions for each tag across the pass over \nlmfour{} meaning
that the instance index on the x axis is the number of instances that have been
processed at that point. Note that the plotted tags will in general not be the
only target tags in an MRL. E.g., \osmtag{healthcare=optometrist} usually occurs
in a union with \osmtag{shop=optician} and \osmtag{sport=cricket} often occurs
in an intersection with either \osmtag{leisure=stadium} or
\osmtag{leisure=pitch}.

The results of the longitudinal analysis are mixed: There are several tags like
\osmtag{shop=tattoo}, \osmtag{sport=cricket}, \osmtag{sport=ice_skating} and
\osmtag{cuisine=mexican} where the predictions feature a noticeable improvement
over time, but there are also tags like \osmtag{shop=butcher} where there is no
clear improvement. This discrepancy can be understood by taking a closer look at
the difference between queries involving \osmtag{tattoo} and \osmtag{butcher}.
While the NL queries asking for the former all involve the token \nl{tattoo}
(\nl{i want a tattoo …}, \nl{… closest tattoo studio …}, etc.) and the MRL
queries’ only target tag is \osmtag{shop=tattoo}, the situation is more complex
for butchers on both the NL and the MRL side: As shown in
Figure~\ref{fig:butcher-nl}, the NL queries may directly ask for butchers, but
they may also ask for places to buy meat or even sausages. As a further
complication, the MRL queries when asking for butchers and meat shops will only
target the \osmtag{shop=butcher} tag, but MRL queries when asking for places to
buy meat or sausages will additionally target the \osmtag{shop=supermarket} tag.

In the case of \osmtag{amenity=fire_station}, the model fails to correctly use
the tag for a fourth time after three successes and instead uses the
non-existent \osmtag{amenity=} \mrl{firestation} (dropping the underscore). This
is one of several cases that illustrate the brittleness of the character-based
model.

\begin{figure}[h]
  \centering
  \begin{lstlisting}[style=MyNL]
(@\textcolor{mplRed}{When is the meat shop next to Residencial Ibirapuera open?}@)
(@\textcolor{mplRed}{meat shops in Auxerre (canton d'Auxerre-3)}@)
(@\textcolor{mplGreen}{show me butchers in Marchais-Beton}@)
(@\textcolor{mplRed}{where can I buy meat in Babiny I?}@)
(@\textcolor{mplGreen}{Are there any butcher shops in Sommerland that is accessible by wheelchair?}@)
(@\textcolor{mplBlue}{how many butchers in cukurambar mahallesi in Podgaje}@)
(@\textcolor{mplRed}{where can I buy sausages in Vlkov u Jaroměře?}@)
(@\textcolor{mplRed}{where can i buy meat in rennes}@)
  \end{lstlisting}
  \caption[NL queries for butchers]{NL queries in the \nlmfour{} training set
    that features \osmtag{shop=butcher} in the corresponding MRL. Note that the
    blue instance with butchers in Podgaje is obscured by overlapping in
    Figure~\ref{fig:online-analysis}.}
  \label{fig:butcher-nl}
\end{figure}

%%% Local Variables:
%%% coding: utf-8
%%% mode: latex
%%% TeX-engine: luatex
%%% TeX-parse-self: t
%%% TeX-command-extra-options: "-shell-escape"
%%% TeX-master: "../thesis"
%%% End: