\chapter{Related Work}
\label{ch:related-work}

\section{Question Answering by Semantic Parsing}

With the advent of the computer age, there also arose interest in leveraging the
digitally stored information for automatically answering natural language
questions with first systems being developed as early as the 1960s
\parencite[e.g.][]{green-1961}. The field of question answering (QA) can be
genereally divided into \emph{open-domain QA}, which concerns itself with
answering arbitrary questions based on large quantities of natural language text
or other unstructured information by employing techniques from information
retrieval, and \emph{knowledge-based QA}, which takes advantage of structured
information in order to answer questions about specific information that is
stored in that knowledge base \parencites[cf.][]{molla-2007}{jurafsky-2021}. The
work in this thesis is an instance of the latter.

In the most common variant of knowledge-based QA, a natural language query is
parsed into a machine-readable logical form for representing the meaning
(\emph{semantic parsing}), which can then be used to extract the answer from the
knowledge base. Notable datasets for this task include the Air Travel
Information System (ATIS) dataset \parencite{hemphill-1990}, which maps
\num{5280} questions about flights in the USA to a representation in SQL, and
GeoQuery, which maps \num{877} questions about US geography to lambda-calculus
based representations in Prolog \parencite{zelle-1996}. While these two are
small datasets specialized on one domain each, the crowd-sourced WikiSQL dataset
introduced by \textcite{zhong-2017} comprises \num{80654} questions on numerous
different databases, but all of their queries are very simple and include no
advanced SQL operators.

With their dataset called Spider, \textcite{yu-2018} introduced the first
text-to-SQL dataset that features a large number of different tables as well as
complex SQL queries. They present initial results on their dataset using
sequence-to-sequence models like attention-based RNNs as well as more rigid
slot-filling models based on SQLNet \parencite{xu-2017}, which fill pre-defined
places in the SQL query. The latter perform better in their experiments.

Since \textcite{hwang-2019} showed that using BERT \parencite{devlin-2019} for
encoding query and database schema in text-to-SQL task is highly effective on
WikiSQL, most recent systems use models based on BERT also on the Spider task
\parencites{shaw-2020}{wang-2020}{lin-2020} with some even taking advantage of
the content of the database \parencites{wang-2020}{lin-2020} to further improve
performance.

\section{Online Learning and Domain Adaptation}

\subsection{Online Learning}

Neural machine translation models \(p_{\theta}\) are usually
\parencite[376]{stahlberg-2020} trained with the goal of minimizing the expected
cross-entropy loss on the training set \((X, Y)_{\text{train}}\) consisting of a
set of source sentences \(X\) and a set of target sentences \(Y\) by taking
steps \(\Delta \theta\) in the opposite direction of the gradient:

\begin{align}
  \theta^* &= \argmin_{\theta} \E_{(x, y) \sim (X, Y)_{\text{train}}} -\log p_{\theta}(y|x)\\
  \Delta \theta &\propto \nabla \E_{(x, y) \sim (X, Y)_{\text{train}}} -\log p_{\theta}(y|x)\\
  &\propto \E_{(x, y) \sim (X, Y)_{\text{train}}} -\nabla \log p_{\theta}(y|x)
\end{align}

This explicit method – also called \emph{batch gradient descent} as an instance
of \emph{batch learning} \parencites[275]{goodfellow-2016}[100]{murphy-2021} –
requires making a prediction for every instance of the training set, which makes
it prohibitively computationally expensive. Instead, \emph{stochastic gradient
  descent} (SGD) – as an instance of \emph{online learning}
\parencites[275]{goodfellow-2016}[100]{murphy-2021} – approximates the gradient
by sampling a single instance \((x,y)\) from the training set and updating the
parameters based on this instance:

\begin{align}
  (x,y) &\sim (X,Y)_{\text{train}}\\
  \Delta \theta &\propto -\nabla \log p_{\theta}(y|x)\\
\end{align}

With SGD, the model parameters can be updated much more often, but the variance
of the gradient is also larger. In practice, minibatches are used as a
compromise when training a model on an already-prepared dataset. However, in the
case where the dataset becomes available only one instance at a time, the online
learning setup is still useful. A prominent usecase is post-editing of machine
translation outputs. Following the definition of \textcite{ortiz-martinez-2016},
online learning from post-editing can be operationalized like this:

\begin{enumerate}
\item An MT system receives a source sentence \(x\).
\item The system makes a prediction \(\hat{y}\) for the target sentence.
\item A user reviews \(\hat{y}\), adjusts it and presents the system with the
  correct translation \(y\).
\item The MT system is updated by learning from \(y\).
\end{enumerate}

This procedure is especially useful for adapting a system pre-trained on general
data to another domain. In statistical machine translation (SMT), online
learning research has first focused on adjusting the weights of the log-linear
model \parencites(e.g.)(){liang-2006}{arun-2007}{watanabe-2007}. Later,
\textcites{ortiz-martinez-2010}{ortiz-martinez-2016} employed online learning in
the scenarios of post-editing and interactive machine translation (IMT;
\cites{casacuberta-2009}{barrachina-2009}) for adjusting also the model features
themselves.

For NMT, \textcite{turchi-2017} simulated online learning with an attentional
encoder-decoder network finding that simply updating by doing an SGD step with
the post-edited instance is superior to a more complicated approach in which
they added an additional training step after receiving each source sentence but
\emph{before} predicting the target sentence. They also found that doing five
training iterations per post-edited instance is actually worse than doing only
one iteration. Very interestingly, vanilla SGD outperforms Adagrad, Adadelta and
Adam in their experiments. % TODO: Cite Ada*

At the same time, \textcite{peris-2017} performed very similar work, in which
they compared gradient descent optimization algorithms with more complicated
passive-aggressive update rules using subgradient methods. Gradient descent
optimizers performed significantly better, although vanilla SGD was inferior to
adaptive gradient update rules, out of which Adadelta and Adam performed best.

\textcite{peris-2019} continued this work for post-editing and IMT scenarios and
found Adadelta to be the best and also the most stable optimizer with regard to
varying the learning rate. They demonstrate that online learning can be
successfully used to enhance the performance not only of out-of-domain systems,
but also of systems pre-trained with a limited amount of in-domain data.

While the work of \textcites{turchi-2017}{peris-2017}{peris-2019} was done using
simulations, \textcite{karimova-2018} conducted an actual experiment with
students of translation studies post-editing NMT outputs, in which they found
that online adaptation was able to improve both BLEU score and key-stroke and
mouse-action ratio \parencite[for KSMR cf.][]{barrachina-2009}.

\subsection{Learning from Weak User Feedback}

A related line of NMT research attempts to improve a pre-trained system with a
weaker form of user feedback: Instead of a user post-editing a target sentence,
they can give feedback in the form of scalar rewards about the quality of a
translation or even specific parts of it, e.g.\ by rating it from 0 to 10. This
has the advantage of requiring less user effort and skill, but it is more
difficult for a model to correct its predicted translation due to the lack of a
gold translation. This poses a particular challenge in tasks with a very large
output space like machine translation.

Scalar feedback can be leveraged with reinforcement learning techniques, which
was done by \textcite{kreutzer-2017}, who simulated user feedback with
sentence-level gGLEU \parencite{wu-2016} obtained from out-of-domain reference
translations. In their comparison of domain adaptation via fully-supervised
fine-tuning and learning from the gGLEU \enquote{rewards} using expected loss
minimization (which is in essence the REINFORCE algorithm by
\textcite{williams-1992}), they found that learning from weak feedback is a
viable method of domain adaptation, even though the fully-supervised approach
naturally yielded better results. They also showed that performance on the
original data used for pre-training deteriorated in both scenarios, but more so
with fully-supervised fine-tuning. \textcite{nguyen-2017} employ the advantage
actor-critic algorithm \parencite{mnih-2016} in a similar scenario with one
difference of introducing noise and skew in their reward signals in order to
simulate actual user feedback. They also do not perform domain adaptation, but
\emph{fine-tune} on a separate part of the training set.

In contrast to those systems, which simulate a sequence-level reward signal
after predicting a complete translation, \textcite{lam-2018} use an advantage
actor-critic NMT system in an IMT-style setting where feedback is simulated for
\emph{partial} translations when the NMT model is uncertain about its
prediction. Unsurprisingly, they observed that the more granular feedback yields
improvements over the sentence-level feedback used in \textcite{nguyen-2017}.

Online methods are of limited use in production systems where the risk of
deteriorating performance cannot be taken, especially in the light of possibly
adversarial actors. Therefore, user feedback is usually logged for later
learning, which is unproblematic for post-edits since they can serve as
self-contained training instances not much different from reference translations
in specially-prepared training sets. However, weaker user feedback in the form
of ratings is tied to the system’s original prediction, which complicates
learning from it with another system. \textcite{lawrence-2017} used a
control-variate-based smoothing technique called \emph{deterministic propensity
  matching} to leverage this kind of logged feedback for offline learning with
an SMT system. Later, \textcite{lawrence-2018} applied this approach also to
improving an NMT-based parser on the \nlmapstwo{} dataset, which is discussed
later in this thesis. They were able to learn from both sequence- and
token-based feedback (both simulated and actual human), but the more granular
token-based feedback proved superior.

\textcite{kreutzer-2020a} conducted an offline learning NMT experiment with
human annotators who corrected wrong translations in one scenario and only
marked erroneous passages in another scenario. They found comparable
improvements over the baseline in both scenarios, but the error markings took
significantly less effort to collect.

For an overview of human feedback in reinforcement learning for NLP, see the
work of \textcite{kreutzer-2020b}.

\subsection{Domain Adaptation in Semantic Parsing}

Domain Adaptation is especially interesting for custom semantic parsing systems
on new domains since creating custom datasets involves a lot of expensive
annotation work resulting in small dataset sizes. The goal here is pre-training
a model on a larger dataset that is similar to the custom one in order to reduce
the needed size of the custom dataset or to simply improve performance.
\textcite{kennardi-2019} pre-trained an attention-based RNN on the ATIS dataset
and then fine-tuned the pre-trained model on subsets of the GeoQuery dataset
showing that especially for small subsets (i.e.\ small target domain datasets)
pre-training on ATIS improved the performance.

Instead of pre-training a semantic parsing model on a larger semantic parsing
dataset, \textcite{chen-2020} employ a system that takes advantage of
pre-trained language representations in the form of BART \parencite{lewis-2020},
a BERT-inspired encoder-decoder setup trained by denoising artificially
corrupted text. They show that the BART-based parser incorporating pre-trained
language representations outperforms an LSTM-based \parencite{hochreiter-1997}
parser trained from scratch – an effect that is especially pronounced when only
a subset of the data from the target domain is used.

\section{OpenStreetMap Query Systems}

\subsection{OpenStreetMap and its Ecosystem}
\label{sec:osm}

In a crowd-sourcing approach similar to Wikipedia’s, the
OpenStreetMap\footciteonline{openstreetmap} (OSM) project aims to create a map
of the world by letting users contribute missing data, ranging from low-granular
objects like forests or streets to high-granular objects like benches or
information boards and even including non-geographical information like opening
hours of stores or the types of cuisine available in restaurants. The
OpenStreetMap Foundation makes the map data available under the Open Data
Commons Open Database License\footciteonline{odbl} effectively allowing the
usage of the data for any project but requiring that any extensions of the data
are shared under the same license again.

OpenStreetMap data is made up of three different elements: Nodes, ways (ordered
lists of nodes) and relations (groups of elements). The elements’ meaning is
derived from the tags that are added to them. For instance, an Italian
restaurant that has vegan options and is wheelchair-accessible may be tagged as
a node with the following tags:

\begin{itemize}
\item \osmtag{amenity=restaurant}
\item \osmtag{cuisine=italian}
\item \osmtag{diet:vegan=yes}
\item \osmtag{diet:vegetarian=yes}
\item \osmtag{wheelchair=yes}
\item \osmtag{opening_hours=Mo-Sa 11:30-22:00}
\item \osmtag{website=https://restaurant.example.com/}
\end{itemize}

The OpenStreetMap database can be queried in a number of ways, the most
prevalent one of which is via Geocoders such as
Nominatim\footciteonline{nominatim}. They allow the database being queried by
the name or address of an object in \emph{forward geocoding} or by its
geographic coordinates in \emph{reverse geocoding}.

For querying by more than name or address, there are two specialized systems:
Sophox\footciteonlinetwo{sophox}{The official website at
  \url{https://sophox.org} is offline as of December 2020.} and the Overpass API
\footciteonline{overpass-api}, which can be used most conveniently via the
Overpass Turbo \footciteonline{overpass-turbo} interface. The Overpass API
allows queries using an XML-like language or – more prominently – its custom
Overpass Query Language (Overpass QL). The question \nl{Which Italian
  restaurants in Berlin are wheelchair-accessible?} could be expressed with the
following Overpass Query

\begin{lstlisting}[style=MyOverpassQL,title={Overpass QL for wheelchair-accessible restaurants in Heidelberg}]
(area[name=Heidelberg];) -> .a;
nwr[amenity=restaurant][wheelchair=yes](area.a);
out geom;
\end{lstlisting}

\subsection{NLMaps}
\label{sec:nlmaps}

The open license as well as the diverse and rich information available in the
data make OSM a promising candidate for the foundation of an information
retrieval system about geo-related questions. The first step in this direction
was made by \textcite{haas-2016}, when they released the first version of the
\nlmaps{} dataset a \textquote[][.]{corpus consisting of 2,380 questions about
  geographical facts that can be answered with the [OSM]
  database}\footciteonline{nlmaps} Each question is provided as a natural
language (NL) query in English and in German and as its rendering in a custom
machine-readable language (MRL). The dataset can be used to develop a parser for
parsing an NL query into its corresponding MRL query, which can then be used to
extract the answer to the question from the OSM database.

In two subsequent works, Lawrence\footnote{Carolin Haas changed her name to
  Carolin Lawrence in 2016.} and Riezler
\parencites*{lawrence-2016}{lawrence-2018} expanded the English
part\footnote{\nlmapstwo{} is not available in German.} of the dataset to
include more NL-MRL pairs and by extension also more word types and OSM tags.
Table~\ref{tab:nlmaps-v1-v2-stats} shows key data about the size of the extended
dataset. Table~\ref{tab:nlmapsv2-splits} shows the size of the dataset splits.
In contrast to the first version, the NL-MRL pairs in this extended version were
created with a templating approach, which made use of a
table\footciteonlinetwo{special-phrases}{It is not known which version of the
  table was used for generating the \nlmapstwo{} dataset.} mapping natural
language expressions (such as \emph{restaurant}) to OSM tags (such as
\osmtag{amenity=restaurant}).

\begin{table}[ht!]
  \centering
  \begin{tabular}[h]{lll}
    \toprule
    & \nlmapsone{} & \nlmapstwo{}\\
    \midrule
    Instances & \num{2380} & \num{28609}\\
    Tokens & \num{25906} & \num{202088}\\
    Types & \num{1002} & \num{8710}\\
    Avg. Tokens per NL & \num{10.88} & \num{7.06}\\
    Distinct Tags & \num{477} & \num{6582}\\
    \bottomrule
  \end{tabular}
  \caption[\nlmapstwo{} statistics]{Numeric information about \nlmapsone{} and
    \nlmapstwo{}. The table is reproduced from \textcite{lawrence-2018}.}
  \label{tab:nlmaps-v1-v2-stats}
\end{table}

\begin{table}[ht!]
  \centering
  \begin{tabular}[h]{ll}
    \toprule
    Set split & \nlmapstwo{}\\
    \midrule
    train & \num{16172}\\
    dev & \num{1843}\\
    test & \num{10594}\\
    \bottomrule
  \end{tabular}
  \caption[\nlmapstwo{} splits]{Split sizes in the \nlmapstwo{} dataset.}
  \label{tab:nlmapsv2-splits}
\end{table}

In addition to the NL and the MRL queries, the dataset includes a linearized
(LIN) version of the MRL query. This is a formally equivalent variant of the MRL
query that avoids parentheses and commata by specifying each operator’s arity
instead. For further information on this,
\textcites(cf.)(){andreas-2013}{haas-2016}. All parsing models discussed in this
thesis parse the NL query into the LIN query, which can be converted into the
MRL query for retrieving the result from the OSM database.

All of the question and query variants are also provided in a version where the
locations and the points of interest are replaced by generic
\lstinline!_LOCATION! and \lstinline!_POI! tokens, respectively. This is
intended to simplify training a parser model which relies on an external Named
Entity Recognition (NER) component for the named entities.
Figure~\ref{fig:nlmaps-v2-query-variants} shows an overview of the available
variants.

\begin{figure}[h]
  \centering
    \begin{minipage}{0.48\textwidth}
      \begin{lstlisting}[style=MyMRL,title={Unmasked MRL},basicstyle={\ttfamily\scriptsize}]
query(
  around(
    center(
      area(keyval('name','München)),
      nwr(keyval('name','Super Cut'))
    ),
    search(
      nwr(keyval('amenity','post_box'))
    ),
    maxdist(DIST_INTOWN)
  ),
  qtype(latlong)
)
      \end{lstlisting}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
      \begin{lstlisting}[style=MyMRL,title={Masked MRL},basicstyle={\ttfamily\scriptsize{}}]
query(
  around(
    center(
      area(keyval('name','(@\textcolor{blue}{\_LOCATION}@)')),
      nwr(keyval('name','(@\textcolor{blue}{\_POI}@)'))
    ),
    search(
      nwr(keyval('amenity','post_box'))
    ),
    maxdist(DIST_INTOWN)
  ),
  qtype(latlong)
)
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
      \begin{lstlisting}[style=MyLin,title={Unmasked LIN},basicstyle={\ttfamily\scriptsize}]
query@2
  around@3
    center@2
      area@1 keyval@2 name@0 München@s
      nwr@1 keyval@2 name@0 Super€Cut@s
    search@1
      nwr@1 keyval@2 amenity@0 post_box@s
    maxdist@1 DIST_INTOWN@0
  qtype@1 latlong@0
      \end{lstlisting}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
      \begin{lstlisting}[style=MyLin,title={Masked LIN},basicstyle={\ttfamily\scriptsize{}}]
query@2
  around@3
    center@2
      area@1 keyval@2 name@0 (@\textcolor{blue}{\_LOCATION}@)@s
      nwr@1 keyval@2 name@0 (@\textcolor{blue}{\_POI}@)@s
    search@1
      nwr@1 keyval@2 amenity@0 post_box@s
    maxdist@1 DIST_INTOWN@0
  qtype@1 latlong@0
      \end{lstlisting}
    \end{minipage}
    \caption[\nlmapstwo{} query variants]{MRL and LIN queries for the NL query
      \nl{Where Post Boxes near Super Cut in München} in their unmasked and
      masked form. The instance is taken from \nlmapstwo{}.}
  \label{fig:nlmaps-v2-query-variants}
\end{figure}

\subsubsection{\nlmaps{} Evaluation}

There are two methods of evaluating a model’s predictions on an \nlmaps{}
dataset: Comparing the predicted MRLs with the gold MRLs or comparing the
results retrieved by interpreting the MRLs. The latter method defines precision,
recall and \(F_1\) score, where \(F_1\) is the measure that is usually reported.

\begin{align*}
  \text{precision} &= \frac{\text{number of correct answers}}{\text{number of MRLs that yield an answer}}\\
  \text{recall} &= \frac{\text{number of correct answers}}{\text{number of all NL-MRL pairs}}\\
  F_1 &= 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
\end{align*}

This thesis takes the stand that comparing the results of queries is not
appropriate because of two major reasons: First, the result of two MRLs may be
identical by chance, especially when asking whether something exists or how many
of something there are. Second, such an evaluation is dependent on the versions
of the OSM database and the version of the software interpreting the MRL making
reported scores difficult to reproduce. Additionally, retrieving results takes
up a large amount of time and computing resources, which renders this evaluation
method unsuitable for validation during training.

Therefore, the models in this thesis are compared using exact match accuracy on
MRLs. Theoretically, this method has the problem that MRLs can be semantically
equivalent but syntactically different by switching the order of OSM tags or
values (e.g. \mrl{keyval('diet:vegetarian',} \mrl{or('yes','only'))} vs.
\mrl{keyval('diet:} \mrl{vegetarian',} \mrl{or('only,'yes))}). In practice
however, we ensure that all MRLs used for training are in a canonical form by
sorting tags and values alphabetically. This way, switched order does virtually
not occur in the models of this thesis.

\begin{align*}
  \text{accuracy} &= \frac{\text{number of system MRLs perfectly matching the gold MRL}}{\text{number of all NL-MRL pairs}}
\end{align*}

\subsubsection{Previous Results on \nlmapstwo{}}

\textcite{lawrence-2018} trained a GRU-based encoder-decoder model
\parencite{cho-2014} with Bahdanau attention \parencite{bahdanau-2015} on
\nlmapstwo{}, once without masking the named entities and once with masking
them. The model trained on the masked version is accompanied by an NER model.
\textcite{staniek-2020} trained a similar model for comparison with
\textcite{lawrence-2018}, once as a token-based RNN and once as a
character-based RNN, both without masking the named entities. The results are
shown in Table~\ref{tab:lawrence-staniek-results}. In essence, they show that it
is easy enough for the character-based model to copy the named entities from the
source to the target so that a separate NER model does not improve the results.

\begin{table}[ht!]
  \centering
  \begin{tabular}{lrr}
    \toprule
    Model & unmasked & masked + NER\\
    \midrule
    \textcite{lawrence-2018} (token) & \num{.804} & \num{.901}\\
    \textcite{staniek-2020} (token) & \num{.834} & ---\\
    \textcite{staniek-2020} (character) & \bfnum{.938} & ---\\
    \bottomrule
  \end{tabular}
  \caption[Previous NLMaps results]{\(F_1\) score after retrieving query results
    of models by \textcite{lawrence-2018} and \textcite{staniek-2020} on
    \nlmapstwo{}}
  \label{tab:lawrence-staniek-results}
\end{table}

Even though \citeauthor{staniek-2020} achieves an \(F_1\) score of \SI{93.8}{\%}
and an accuracy of \SI{89.8}{\%}\footnote{The accuracy is not reported in the
  original work, but \citeauthor{staniek-2020} kindly made his model
  available.}, the task of parsing NL queries is not solved, at all. The high
performance is the result of a number of shortcomings in the \nlmapstwo{}
dataset, a part of which has already been discussed by \citeauthor{staniek-2020}
and which are investigated in more detail in
Chapter~\ref{ch:nlmaps-improvement}.

%%% Local Variables:
%%% coding: utf-8
%%% mode: latex
%%% TeX-engine: luatex
%%% TeX-parse-self: t
%%% TeX-command-extra-options: "-shell-escape"
%%% TeX-master: "../thesis"
%%% End:
